import gradio as gr
import cv2
import numpy as np
import tempfile
import os
import zipfile

MAX_SIZE = 1920

# ===== Ïú†Ìã∏Î¶¨Ìã∞ =====
def resize_if_needed(img):
    h, w = img.shape[:2]
    if max(h, w) > MAX_SIZE:
        scale = MAX_SIZE / max(h, w)
        img = cv2.resize(img, None, fx=scale, fy=scale, interpolation=cv2.INTER_LANCZOS4)
    return img

def resize_and_crop_to_match(img1, img2):
    h1, w1 = img1.shape[:2]
    h2, w2 = img2.shape[:2]
    ratio1, ratio2 = w1/h1, w2/h2
    
    if ratio1 > ratio2:
        new_w = int(h2 * ratio1)
        new_h = h2
    else:
        new_w = w2
        new_h = int(w2 / ratio1)
    
    img1_resized = cv2.resize(img1, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)
    sx, sy = (new_w - w2) // 2, (new_h - h2) // 2
    return img1_resized[sy:sy+h2, sx:sx+w2]

def match_brightness(img1, img2):
    lab1 = cv2.cvtColor(img1, cv2.COLOR_BGR2LAB).astype(np.float32)
    lab2 = cv2.cvtColor(img2, cv2.COLOR_BGR2LAB).astype(np.float32)
    avg = (np.mean(lab1[:,:,0]) + np.mean(lab2[:,:,0])) / 2
    lab1[:,:,0] = np.clip(lab1[:,:,0] + (avg - np.mean(lab1[:,:,0])), 0, 255)
    lab2[:,:,0] = np.clip(lab2[:,:,0] + (avg - np.mean(lab2[:,:,0])), 0, 255)
    return cv2.cvtColor(lab1.astype(np.uint8), cv2.COLOR_LAB2BGR), cv2.cvtColor(lab2.astype(np.uint8), cv2.COLOR_LAB2BGR)

def add_logo(frame, logo, margin_ratio=0.03, width_ratio=0.27):
    if logo is None:
        return frame
    h, w = frame.shape[:2]
    logo_w = int(w * width_ratio)
    logo_h = int(logo.shape[0] * (logo_w / logo.shape[1]))
    logo_resized = cv2.resize(logo, (logo_w, logo_h), interpolation=cv2.INTER_LANCZOS4)
    margin = int(w * margin_ratio)
    
    if logo_resized.shape[2] == 4:
        alpha = logo_resized[:, :, 3:4] / 255.0
        roi = frame[margin:margin+logo_h, margin:margin+logo_w]
        frame[margin:margin+logo_h, margin:margin+logo_w] = (logo_resized[:,:,:3] * alpha + roi * (1-alpha)).astype(np.uint8)
    else:
        frame[margin:margin+logo_h, margin:margin+logo_w] = logo_resized
    return frame

def add_label(img, text, position='top'):
    h, w = img.shape[:2]
    font = cv2.FONT_HERSHEY_SIMPLEX
    scale = max(0.7, min(w, h) / 900)
    thickness = max(2, int(scale * 2))
    (tw, th), _ = cv2.getTextSize(text, font, scale, thickness)
    pad = 8
    bx = (w - tw - pad*2) // 2
    by = 12 if position == 'top' else h - th - pad*2 - 12
    cv2.rectangle(img, (bx, by), (bx + tw + pad*2, by + th + pad*2), (0,0,0), -1)
    cv2.putText(img, text, (bx + pad, by + th + pad), font, scale, (255,255,255), thickness)
    return img

# ===== Í∞êÏßÄ =====
def detect_face(img):
    """MediaPipe ÏñºÍµ¥ Í∞êÏßÄ ‚Üí ÏïàÎ™®/Î∞òÎ™® ÌåêÎ≥Ñ"""
    try:
        import mediapipe as mp
        with mp.solutions.face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.3) as fm:
            results = fm.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
            if not results.multi_face_landmarks:
                return None
            
            lm = results.multi_face_landmarks[0].landmark
            h, w = img.shape[:2]
            
            # Îàà Ï¢åÌëú
            left_eye = np.mean([[lm[i].x*w, lm[i].y*h] for i in [33,133,160,159,158,144,145,153]], axis=0)
            right_eye = np.mean([[lm[i].x*w, lm[i].y*h] for i in [362,263,387,386,385,373,374,380]], axis=0)
            
            eye_dist = np.linalg.norm(right_eye - left_eye)
            eye_angle = np.arctan2(right_eye[1]-left_eye[1], right_eye[0]-left_eye[0])
            eye_center = (left_eye + right_eye) / 2
            
            # Ï†ïÏ§ëÏÑ† (ÏΩîÎÅù ~ ÌÑ±)
            nose = np.array([lm[4].x*w, lm[4].y*h])
            chin = np.array([lm[152].x*w, lm[152].y*h])
            upper_lip = np.array([lm[0].x*w, lm[0].y*h])
            lower_lip = np.array([lm[17].x*w, lm[17].y*h])
            
            midline_len = np.linalg.norm(chin - nose)
            midline_angle = np.arctan2(chin[1]-nose[1], chin[0]-nose[0])
            midline_center = (upper_lip + lower_lip) / 2
            
            # ÏïàÎ™®/Î∞òÎ™® ÌåêÎ≥Ñ (ÏñºÍµ¥ Ï¢ÖÌö°ÎπÑ)
            face_pts = [lm[i] for i in [10, 152, 234, 454]]
            xs = [p.x*w for p in face_pts]
            ys = [p.y*h for p in face_pts]
            aspect = (max(ys)-min(ys)) / (max(xs)-min(xs)) if max(xs)!=min(xs) else 1
            
            is_half = aspect < 0.9
            
            return {
                'type': 'half_face' if is_half else 'full_face',
                'eye_dist': eye_dist, 'eye_angle': eye_angle, 'eye_center': eye_center,
                'midline_len': midline_len, 'midline_angle': midline_angle, 'midline_center': midline_center
            }
    except Exception as e:
        print(f"Face detection error: {e}")
        return None

def detect_oral(img):
    """Íµ¨Í∞ï(ÏπòÏïÑ) Í∞êÏßÄ - Otsu Í∏∞Î∞ò"""
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    h, w = img.shape[:2]
    
    blurred = cv2.GaussianBlur(gray, (5,5), 0)
    _, mask = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    
    # Ï§ëÏïô ÏòÅÏó≠Îßå
    roi = np.zeros_like(mask)
    m = 0.12
    roi[int(h*m):int(h*(1-m)), int(w*m):int(w*(1-m))] = 255
    mask = cv2.bitwise_and(mask, roi)
    
    kernel = np.ones((5,5), np.uint8)
    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
    
    # ÏÉÅÌïòÏïÖ Í≤ΩÍ≥Ñ Ï∞æÍ∏∞
    row_sum = np.convolve(np.sum(mask>0, axis=1), np.ones(11)/11, mode='same')
    search = row_sum[int(h*0.35):int(h*0.65)]
    occlusal_y = int(h*0.35) + np.argmin(search) if len(search) > 0 else h//2
    
    # ÏÉÅÏïÖ ÏòÅÏó≠
    upper = mask.copy()
    upper[occlusal_y:, :] = 0
    
    contours, _ = cv2.findContours(upper, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours:
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    if not contours:
        return None
    
    largest = max(contours, key=cv2.contourArea)
    if cv2.contourArea(largest) < w*h*0.01:
        return None
    
    x, y, bw, bh = cv2.boundingRect(largest)
    M = cv2.moments(largest)
    cx = M['m10']/M['m00'] if M['m00'] else x+bw/2
    cy = M['m01']/M['m00'] if M['m00'] else y+bh/2
    
    return {'type': 'oral', 'cx': cx, 'cy': cy, 'width': bw, 'height': bh}

def detect_features(img):
    """ÌÜµÌï© Í∞êÏßÄ: ÏñºÍµ¥ ‚Üí Íµ¨Í∞ï ‚Üí Ìè¥Î∞±"""
    face = detect_face(img)
    if face:
        return face
    
    oral = detect_oral(img)
    if oral:
        return oral
    
    h, w = img.shape[:2]
    return {'type': 'fallback', 'cx': w/2, 'cy': h/2, 'width': w*0.5}

# ===== Ï†ïÎ†¨ =====
def align_balanced(before, after, bi, ai):
    """Í∑†Ìòï Ïä§ÏºÄÏùºÎßÅ: Îëê Ïù¥ÎØ∏ÏßÄÎ•º Ï§ëÍ∞Ñ ÌÅ¨Í∏∞Î°ú Ï†ïÎ†¨"""
    h, w = before.shape[:2]
    target = np.array([w/2, h/2])
    
    b_type, a_type = bi['type'], ai['type']
    
    # ÏïàÎ™® (Full Face)
    if b_type == 'full_face' and a_type == 'full_face':
        ratio = ai['eye_dist'] / bi['eye_dist']
        ratio = np.clip(ratio, 0.5, 2.0)
        mid_scale = np.sqrt(ratio)
        b_scale = np.clip(mid_scale, 0.8, 1.25)
        a_scale = np.clip(1/mid_scale, 0.8, 1.25)
        
        angle_diff = ai['eye_angle'] - bi['eye_angle']
        b_angle = np.degrees(angle_diff/2)
        a_angle = -np.degrees(angle_diff/2)
        
        b_anchor = bi['eye_center']
        a_anchor = ai['eye_center']
        align_type = "ÏïàÎ™®"
    
    # Î∞òÎ™® (Half Face)
    elif b_type == 'half_face' and a_type == 'half_face':
        ratio = ai['midline_len'] / bi['midline_len']
        ratio = np.clip(ratio, 0.5, 2.0)
        mid_scale = np.sqrt(ratio)
        b_scale = np.clip(mid_scale, 0.8, 1.25)
        a_scale = np.clip(1/mid_scale, 0.8, 1.25)
        
        angle_diff = ai['midline_angle'] - bi['midline_angle']
        b_angle = np.degrees(angle_diff/2)
        a_angle = -np.degrees(angle_diff/2)
        
        b_anchor = bi['midline_center']
        a_anchor = ai['midline_center']
        align_type = "Î∞òÎ™®"
    
    # Íµ¨Í∞ï (Oral)
    elif b_type == 'oral' and a_type == 'oral':
        ratio = ai['width'] / bi['width']
        ratio = np.clip(ratio, 0.5, 2.0)
        mid_scale = np.sqrt(ratio)
        b_scale = np.clip(mid_scale, 0.7, 1.4)
        a_scale = np.clip(1/mid_scale, 0.7, 1.4)
        
        b_angle, a_angle = 0, 0
        b_anchor = np.array([bi['cx'], bi['cy']])
        a_anchor = np.array([ai['cx'], ai['cy']])
        align_type = "Íµ¨Í∞ï"
    
    # ÌòºÌï© ÎòêÎäî Ìè¥Î∞±
    else:
        # Îëò Ï§ë ÌïòÎÇòÎùºÎèÑ oralÏù¥Î©¥ oral Î∞©Ïãù
        if b_type == 'oral' or a_type == 'oral':
            b_w = bi.get('width', w*0.5)
            a_w = ai.get('width', w*0.5)
            ratio = a_w / b_w
            ratio = np.clip(ratio, 0.5, 2.0)
            mid_scale = np.sqrt(ratio)
            b_scale = np.clip(mid_scale, 0.7, 1.4)
            a_scale = np.clip(1/mid_scale, 0.7, 1.4)
            
            b_anchor = np.array([bi.get('cx', w/2), bi.get('cy', h/2)])
            a_anchor = np.array([ai.get('cx', w/2), ai.get('cy', h/2)])
            b_angle, a_angle = 0, 0
            align_type = "Íµ¨Í∞ï(ÌòºÌï©)"
        else:
            # Ìè¥Î∞±: Î≥ÄÌôò ÏóÜÏùå
            return before.copy(), after.copy(), "ÎØ∏Í∞êÏßÄ"
    
    # Î≥ÄÌôò Ï†ÅÏö©
    def transform(img, anchor, angle, scale, target):
        M = cv2.getRotationMatrix2D(tuple(anchor), angle, scale)
        new_anchor = M @ np.array([anchor[0], anchor[1], 1])
        M[0,2] += target[0] - new_anchor[0]
        M[1,2] += target[1] - new_anchor[1]
        return cv2.warpAffine(img, M, (img.shape[1], img.shape[0]), borderMode=cv2.BORDER_REPLICATE)
    
    before_out = transform(before, b_anchor, b_angle, b_scale, target)
    after_out = transform(after, a_anchor, a_angle, a_scale, target)
    
    return before_out, after_out, f"{align_type}({b_scale:.2f}/{a_scale:.2f})"

# ===== Ï≤òÎ¶¨ =====
def process_images(before_img, after_img, logo_img=None):
    """Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ Î∞è Ï†ïÎ†¨"""
    before = cv2.cvtColor(before_img, cv2.COLOR_RGB2BGR)
    after = cv2.cvtColor(after_img, cv2.COLOR_RGB2BGR)
    
    after = resize_if_needed(after)
    before = resize_if_needed(before)
    
    if before.shape[:2] != after.shape[:2]:
        before = resize_and_crop_to_match(before, after)
    
    logo = None
    if logo_img is not None:
        logo = cv2.cvtColor(logo_img, cv2.COLOR_RGBA2BGRA if logo_img.shape[2]==4 else cv2.COLOR_RGB2BGR)
    
    bi = detect_features(before)
    ai = detect_features(after)
    
    before_aligned, after_aligned, align_type = align_balanced(before, after, bi, ai)
    
    # ÌÅ¨Î°≠
    h, w = after_aligned.shape[:2]
    m = 0.05
    bf = before_aligned[int(h*m):int(h*(1-m)), int(w*m):int(w*(1-m))]
    af = after_aligned[int(h*m):int(h*(1-m)), int(w*m):int(w*(1-m))]
    
    bf, af = match_brightness(bf, af)
    
    # 16Î∞∞Ïàò
    th, tw = af.shape[:2]
    fw, fh = (tw//16)*16, (th//16)*16
    bf = cv2.resize(bf, (fw, fh), interpolation=cv2.INTER_LANCZOS4)
    af = cv2.resize(af, (fw, fh), interpolation=cv2.INTER_LANCZOS4)
    
    return bf, af, logo, align_type, fw, fh

# ===== Ï∂úÎ†• ÏÉùÏÑ± =====
def create_dissolve(before_img, after_img, logo_img=None):
    """ÎîîÏ°∏Î∏å ÏòÅÏÉÅ ÏÉùÏÑ±"""
    if before_img is None or after_img is None:
        return None, "Ï†Ñ/ÌõÑ ÏÇ¨ÏßÑÏùÑ Î™®Îëê ÏóÖÎ°úÎìúÌï¥Ï£ºÏÑ∏Ïöî"
    
    try:
        bf, af, logo, align_type, fw, fh = process_images(before_img, after_img, logo_img)
        
        fps = 30
        frames = [39, 12, 39]  # before, dissolve, after
        
        temp = tempfile.mkdtemp()
        path = os.path.join(temp, "dissolve.mp4")
        out = cv2.VideoWriter(path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (fw, fh))
        
        for i in range(sum(frames)):
            if i < frames[0]:
                frame = bf.copy()
            elif i < frames[0] + frames[1]:
                alpha = (i - frames[0]) / frames[1]
                frame = cv2.addWeighted(bf, 1-alpha, af, alpha, 0)
            else:
                frame = af.copy()
            if logo is not None:
                frame = add_logo(frame, logo)
            out.write(frame)
        
        out.release()
        return path, f"{fw}√ó{fh} | 3.0Ï¥à | {align_type}"
    except Exception as e:
        return None, f"Ïò§Î•ò: {e}"

def create_sidebyside(before_img, after_img, logo_img=None):
    """Ï¢åÏö∞ ÎπÑÍµê Ïù¥ÎØ∏ÏßÄ"""
    if before_img is None or after_img is None:
        return None, "Ï†Ñ/ÌõÑ ÏÇ¨ÏßÑÏùÑ Î™®Îëê ÏóÖÎ°úÎìúÌï¥Ï£ºÏÑ∏Ïöî"
    
    try:
        bf, af, logo, align_type, fw, fh = process_images(before_img, after_img, logo_img)
        
        bf_l = add_label(bf.copy(), "BEFORE")
        af_l = add_label(af.copy(), "AFTER")
        
        divider = np.ones((fh, 4, 3), dtype=np.uint8) * 255
        combined = np.hstack([bf_l, divider, af_l])
        
        if logo is not None:
            combined = add_logo(combined, logo, width_ratio=0.15)
        
        return cv2.cvtColor(combined, cv2.COLOR_BGR2RGB), f"{combined.shape[1]}√ó{combined.shape[0]} | {align_type}"
    except Exception as e:
        return None, f"Ïò§Î•ò: {e}"

# ===== Î∞∞Ïπò Ï≤òÎ¶¨ =====
def process_batch(files, output_type, logo_img=None):
    """Î∞∞Ïπò Ï≤òÎ¶¨: Ïó¨Îü¨ ÏåçÏùò B/A Ï≤òÎ¶¨"""
    if not files or len(files) < 2:
        return None, "ÏµúÏÜå 2Í∞ú ÌååÏùº(before/after Ïåç)Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§"
    
    # ÌååÏùº Ï†ïÎ†¨ (Ïù¥Î¶ÑÏàú)
    files = sorted(files, key=lambda x: x.name if hasattr(x, 'name') else x)
    
    if len(files) % 2 != 0:
        return None, "ÌååÏùº Í∞úÏàòÍ∞Ä ÏßùÏàòÏó¨Ïïº Ìï©ÎãàÎã§ (before/after Ïåç)"
    
    temp = tempfile.mkdtemp()
    results = []
    
    for i in range(0, len(files), 2):
        before_path = files[i].name if hasattr(files[i], 'name') else files[i]
        after_path = files[i+1].name if hasattr(files[i+1], 'name') else files[i+1]
        
        before_img = cv2.cvtColor(cv2.imread(before_path), cv2.COLOR_BGR2RGB)
        after_img = cv2.cvtColor(cv2.imread(after_path), cv2.COLOR_BGR2RGB)
        
        pair_idx = i // 2 + 1
        
        if output_type == "ÎîîÏ°∏Î∏å":
            result, status = create_dissolve(before_img, after_img, logo_img)
            if result:
                new_path = os.path.join(temp, f"dissolve_{pair_idx:03d}.mp4")
                os.rename(result, new_path)
                results.append(new_path)
        else:
            result, status = create_sidebyside(before_img, after_img, logo_img)
            if result is not None:
                new_path = os.path.join(temp, f"sidebyside_{pair_idx:03d}.png")
                cv2.imwrite(new_path, cv2.cvtColor(result, cv2.COLOR_RGB2BGR))
                results.append(new_path)
    
    if not results:
        return None, "Ï≤òÎ¶¨Îêú Í≤∞Í≥ºÍ∞Ä ÏóÜÏäµÎãàÎã§"
    
    # ZIPÏúºÎ°ú Î¨∂Í∏∞
    zip_path = os.path.join(temp, "batch_results.zip")
    with zipfile.ZipFile(zip_path, 'w') as zf:
        for r in results:
            zf.write(r, os.path.basename(r))
    
    return zip_path, f"{len(results)}Í∞ú Ï≤òÎ¶¨ ÏôÑÎ£å"

# ===== UI =====
with gr.Blocks(title="Dental B&A", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ü¶∑ ÏπòÍ≥º Ï†ÑÌõÑ ÎπÑÍµê")
    gr.Markdown("ÏïàÎ™®/Î∞òÎ™®/Íµ¨Í∞ï ÏûêÎèô Í∞êÏßÄ ‚Üí Í∑†Ìòï Ïä§ÏºÄÏùº Ï†ïÎ†¨")
    
    with gr.Tabs():
        # Îã®Ïùº Ï≤òÎ¶¨ ÌÉ≠
        with gr.Tab("Îã®Ïùº Ï≤òÎ¶¨"):
            with gr.Row():
                before_input = gr.Image(label="BEFORE", type="numpy")
                after_input = gr.Image(label="AFTER", type="numpy")
            
            with gr.Accordion("Î°úÍ≥† (ÏÑ†ÌÉù)", open=False):
                logo_input = gr.Image(label="PNG Ìà¨Î™Ö Î∞∞Í≤Ω ÏßÄÏõê", type="numpy")
            
            with gr.Row():
                dissolve_btn = gr.Button("üé¨ ÎîîÏ°∏Î∏å ÏòÅÏÉÅ", variant="primary")
                sidebyside_btn = gr.Button("üñºÔ∏è Ï¢åÏö∞ ÎπÑÍµê")
            
            with gr.Row():
                video_out = gr.Video(label="ÏòÅÏÉÅ")
                image_out = gr.Image(label="Ïù¥ÎØ∏ÏßÄ")
            
            status_out = gr.Textbox(label="Ï†ïÎ≥¥")
            
            dissolve_btn.click(create_dissolve, [before_input, after_input, logo_input], [video_out, status_out])
            sidebyside_btn.click(create_sidebyside, [before_input, after_input, logo_input], [image_out, status_out])
        
        # Î∞∞Ïπò Ï≤òÎ¶¨ ÌÉ≠
        with gr.Tab("Î∞∞Ïπò Ï≤òÎ¶¨"):
            gr.Markdown("ÌååÏùºÎ™Ö ÏàúÏÑúÎåÄÎ°ú before/after ÏåçÏúºÎ°ú Ï≤òÎ¶¨Îê©ÎãàÎã§.\n\nÏòà: `01_before.jpg, 01_after.jpg, 02_before.jpg, 02_after.jpg`")
            
            batch_files = gr.File(label="Ïù¥ÎØ∏ÏßÄ ÌååÏùºÎì§", file_count="multiple", file_types=["image"])
            batch_type = gr.Radio(["ÎîîÏ°∏Î∏å", "Ï¢åÏö∞ÎπÑÍµê"], value="ÎîîÏ°∏Î∏å", label="Ï∂úÎ†• ÌòïÏãù")
            batch_logo = gr.Image(label="Î°úÍ≥† (ÏÑ†ÌÉù)", type="numpy")
            
            batch_btn = gr.Button("Î∞∞Ïπò Ï≤òÎ¶¨ ÏãúÏûë", variant="primary")
            batch_out = gr.File(label="Í≤∞Í≥º (ZIP)")
            batch_status = gr.Textbox(label="ÏÉÅÌÉú")
            
            batch_btn.click(process_batch, [batch_files, batch_type, batch_logo], [batch_out, batch_status])

demo.launch()
