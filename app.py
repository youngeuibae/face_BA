import gradio as gr
import cv2
import numpy as np
import tempfile
import os
import subprocess
import json
import zipfile
from pathlib import Path
import re

MAX_SIZE = 1920
VIDEO_MAX_SIZE = 720

def natural_sort_key(s):
    """ìì—° ì •ë ¬ì„ ìœ„í•œ í‚¤ í•¨ìˆ˜"""
    return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', str(s))]

def resize_and_crop_to_match(img1, img2):
    h1, w1 = img1.shape[:2]
    h2, w2 = img2.shape[:2]
    ratio1 = w1 / h1
    ratio2 = w2 / h2
    
    if ratio1 > ratio2:
        new_h = h2
        new_w = int(h2 * ratio1)
    else:
        new_w = w2
        new_h = int(w2 / ratio1)
    
    img1_resized = cv2.resize(img1, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)
    start_x = (new_w - w2) // 2
    start_y = (new_h - h2) // 2
    return img1_resized[start_y:start_y+h2, start_x:start_x+w2]

def resize_if_needed(img, max_size=MAX_SIZE):
    h, w = img.shape[:2]
    if max(h, w) > max_size:
        if w > h:
            new_w = max_size
            new_h = int(h * max_size / w)
        else:
            new_h = max_size
            new_w = int(w * max_size / h)
        img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)
    return img

def get_face_info(img):
    try:
        import mediapipe as mp
        mp_face_mesh = mp.solutions.face_mesh
        with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1,
                                    min_detection_confidence=0.1, min_tracking_confidence=0.1) as face_mesh:
            rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            results = face_mesh.process(rgb)
            if results.multi_face_landmarks:
                lm = results.multi_face_landmarks[0].landmark
                h, w = img.shape[:2]
                
                left_eye_idx = [33, 133, 160, 159, 158, 144, 145, 153]
                left_eye_x = np.mean([lm[i].x for i in left_eye_idx]) * w
                left_eye_y = np.mean([lm[i].y for i in left_eye_idx]) * h
                
                right_eye_idx = [362, 263, 387, 386, 385, 373, 374, 380]
                right_eye_x = np.mean([lm[i].x for i in right_eye_idx]) * w
                right_eye_y = np.mean([lm[i].y for i in right_eye_idx]) * h
                
                eyes_center_x = (left_eye_x + right_eye_x) / 2
                eyes_center_y = (left_eye_y + right_eye_y) / 2
                
                eye_dist = np.sqrt((right_eye_x - left_eye_x)**2 + (right_eye_y - left_eye_y)**2)
                eye_angle = np.arctan2(right_eye_y - left_eye_y, right_eye_x - left_eye_x)
                
                nose_tip = lm[4]
                nose_x, nose_y = nose_tip.x * w, nose_tip.y * h
                
                upper_lip = lm[0]
                upper_lip_x, upper_lip_y = upper_lip.x * w, upper_lip.y * h
                
                lower_lip = lm[17]
                lower_lip_x, lower_lip_y = lower_lip.x * w, lower_lip.y * h
                
                chin = lm[152]
                chin_x, chin_y = chin.x * w, chin.y * h
                
                midline_length = np.sqrt((chin_x - nose_x)**2 + (chin_y - nose_y)**2)
                midline_angle = np.arctan2(chin_y - nose_y, chin_x - nose_x)
                
                midline_center_x = (upper_lip_x + lower_lip_x) / 2
                midline_center_y = (upper_lip_y + lower_lip_y) / 2
                
                idx = [10, 152, 234, 454]
                xs = [lm[i].x * w for i in idx]
                ys = [lm[i].y * h for i in idx]
                face_h = max(ys) - min(ys)
                face_w = max(xs) - min(xs)
                aspect = face_h / face_w if face_w > 0 else 1.0
                is_half_face = aspect < 0.9
                
                return {
                    'eye_dist': eye_dist,
                    'eye_angle': eye_angle,
                    'eyes_center_x': eyes_center_x,
                    'eyes_center_y': eyes_center_y,
                    'midline_length': midline_length,
                    'midline_angle': midline_angle,
                    'midline_center_x': midline_center_x,
                    'midline_center_y': midline_center_y,
                    'is_half_face': is_half_face,
                    'detected': True
                }
    except Exception as e:
        print(f"MediaPipe error: {e}")
    
    h, w = img.shape[:2]
    return {
        'eye_dist': w * 0.3,
        'eye_angle': 0,
        'eyes_center_x': w / 2,
        'eyes_center_y': h * 0.35,
        'midline_length': h * 0.3,
        'midline_angle': np.pi / 2,
        'midline_center_x': w / 2,
        'midline_center_y': h * 0.6,
        'is_half_face': True,
        'detected': False
    }

def match_brightness(img1, img2):
    lab1 = cv2.cvtColor(img1, cv2.COLOR_BGR2LAB).astype(np.float32)
    lab2 = cv2.cvtColor(img2, cv2.COLOR_BGR2LAB).astype(np.float32)
    avg = (np.mean(lab1[:,:,0]) + np.mean(lab2[:,:,0])) / 2
    lab1[:,:,0] = np.clip(lab1[:,:,0] + (avg - np.mean(lab1[:,:,0])), 0, 255)
    lab2[:,:,0] = np.clip(lab2[:,:,0] + (avg - np.mean(lab2[:,:,0])), 0, 255)
    return cv2.cvtColor(lab1.astype(np.uint8), cv2.COLOR_LAB2BGR), cv2.cvtColor(lab2.astype(np.uint8), cv2.COLOR_LAB2BGR)

def add_logo(frame, logo, margin_ratio=0.03, width_ratio=0.27):
    if logo is None:
        return frame
    
    h, w = frame.shape[:2]
    logo_w = int(w * width_ratio)
    logo_h = int(logo.shape[0] * (logo_w / logo.shape[1]))
    logo_resized = cv2.resize(logo, (logo_w, logo_h), interpolation=cv2.INTER_LANCZOS4)
    margin = int(w * margin_ratio)
    
    if len(logo_resized.shape) == 3 and logo_resized.shape[2] == 4:
        alpha = logo_resized[:, :, 3] / 255.0
        alpha = np.stack([alpha, alpha, alpha], axis=2)
        logo_bgr = logo_resized[:, :, :3]
        roi = frame[margin:margin+logo_h, margin:margin+logo_w]
        blended = (logo_bgr * alpha + roi * (1 - alpha)).astype(np.uint8)
        frame[margin:margin+logo_h, margin:margin+logo_w] = blended
    else:
        frame[margin:margin+logo_h, margin:margin+logo_w] = logo_resized
    
    return frame

def align_images(before, after, bi, ai):
    """ì‚¬ì§„ìš©: after ê¸°ì¤€ìœ¼ë¡œ before ì •ë ¬ (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)"""
    is_half = ai['is_half_face']
    
    if is_half:
        scale = ai['midline_length'] / bi['midline_length']
        scale = np.clip(scale, 0.5, 2.0)
        
        angle_rad = ai['midline_angle'] - bi['midline_angle']
        angle_deg = -np.degrees(angle_rad)
        angle_deg = np.clip(angle_deg, -15, 15)
        
        ax, ay = bi['midline_center_x'], bi['midline_center_y']
        tx, ty = ai['midline_center_x'], ai['midline_center_y']
    else:
        scale = ai['eye_dist'] / bi['eye_dist']
        scale = np.clip(scale, 0.5, 2.0)
        
        angle_rad = ai['eye_angle'] - bi['eye_angle']
        angle_deg = -np.degrees(angle_rad)
        angle_deg = np.clip(angle_deg, -15, 15)
        
        ax, ay = bi['eyes_center_x'], bi['eyes_center_y']
        tx, ty = ai['eyes_center_x'], ai['eyes_center_y']
    
    M = cv2.getRotationMatrix2D((ax, ay), angle_deg, scale)
    
    nax = M[0,0]*ax + M[0,1]*ay + M[0,2]
    nay = M[1,0]*ax + M[1,1]*ay + M[1,2]
    
    M[0,2] += tx - nax
    M[1,2] += ty - nay
    
    before_aligned = cv2.warpAffine(before, M, (after.shape[1], after.shape[0]), 
                                     borderMode=cv2.BORDER_CONSTANT, 
                                     borderValue=(255, 255, 255))
    
    return before_aligned, "ë°˜ëª¨(ì •ì¤‘ì„ )" if is_half else "ì•ˆëª¨(ëˆˆ)"


# ============ ì‚¬ì§„ ë¹„êµ ============

def process_single_photo(before_path, after_path, logo=None):
    """ë‹¨ì¼ ì‚¬ì§„ ìŒ ì²˜ë¦¬"""
    before_img = cv2.imread(before_path)
    after_img = cv2.imread(after_path)
    
    if before_img is None or after_img is None:
        return None, "ì´ë¯¸ì§€ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
    
    after = resize_if_needed(after_img)
    before = resize_if_needed(before_img)
    
    if before.shape[:2] != after.shape[:2]:
        before = resize_and_crop_to_match(before, after)
    
    bi = get_face_info(before)
    ai = get_face_info(after)
    
    before_aligned, align_type = align_images(before, after, bi, ai)
    
    h, w = after.shape[:2]
    margin = 0.05
    cx1, cy1 = int(w * margin), int(h * margin)
    cx2, cy2 = int(w * (1 - margin)), int(h * (1 - margin))
    before_crop = before_aligned[cy1:cy2, cx1:cx2]
    after_crop = after[cy1:cy2, cx1:cx2]
    
    bf, af = match_brightness(before_crop, after_crop)
    
    th, tw = af.shape[:2]
    fw, fh = (tw // 16) * 16, (th // 16) * 16
    bf = cv2.resize(bf, (fw, fh), interpolation=cv2.INTER_LANCZOS4)
    af = cv2.resize(af, (fw, fh), interpolation=cv2.INTER_LANCZOS4)
    
    fps = 30
    before_frames, dissolve_frames, after_frames = 39, 12, 39
    total_frames = before_frames + dissolve_frames + after_frames
    
    temp_dir = tempfile.mkdtemp()
    output_path = os.path.join(temp_dir, "output.mp4")
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (fw, fh))
    
    for i in range(total_frames):
        if i < before_frames:
            frame = bf.copy()
        elif i < before_frames + dissolve_frames:
            alpha = (i - before_frames) / dissolve_frames
            frame = cv2.addWeighted(bf, 1-alpha, af, alpha, 0)
        else:
            frame = af.copy()
        if logo is not None:
            frame = add_logo(frame, logo)
        out.write(frame)
    
    out.release()
    
    detect_b = "âœ“" if bi['detected'] else "âœ—"
    detect_a = "âœ“" if ai['detected'] else "âœ—"
    info = f"{fw}Ã—{fh} | {align_type} | ì „({detect_b}) í›„({detect_a})"
    
    return output_path, info

def create_photo_comparison(before_files, after_files, logo_img=None):
    """ì‚¬ì§„ ë°°ì¹˜ ì²˜ë¦¬"""
    if not before_files or not after_files:
        return None, None, "BEFOREì™€ AFTER íŒŒì¼ì„ ëª¨ë‘ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”"
    
    # íŒŒì¼ ì •ë ¬
    before_list = sorted(before_files, key=lambda x: natural_sort_key(Path(x).name))
    after_list = sorted(after_files, key=lambda x: natural_sort_key(Path(x).name))
    
    if len(before_list) != len(after_list):
        return None, None, f"íŒŒì¼ ìˆ˜ê°€ ë‹¤ë¦…ë‹ˆë‹¤: BEFORE {len(before_list)}ê°œ, AFTER {len(after_list)}ê°œ"
    
    # ë¡œê³  ì²˜ë¦¬
    logo = None
    if logo_img is not None:
        if len(logo_img.shape) == 3 and logo_img.shape[2] == 4:
            logo = cv2.cvtColor(logo_img, cv2.COLOR_RGBA2BGRA)
        else:
            logo = cv2.cvtColor(logo_img, cv2.COLOR_RGB2BGR)
    
    results = []
    logs = []
    
    for idx, (bf, af) in enumerate(zip(before_list, after_list)):
        bf_name = Path(bf).stem
        af_name = Path(af).stem
        logs.append(f"[{idx+1}/{len(before_list)}] {bf_name} â†” {af_name}")
        
        try:
            output_path, info = process_single_photo(bf, af, logo)
            if output_path:
                results.append((output_path, f"{bf_name}_{af_name}.mp4"))
                logs[-1] += f" âœ“ {info}"
            else:
                logs[-1] += f" âœ— {info}"
        except Exception as e:
            logs[-1] += f" âœ— ì˜¤ë¥˜: {str(e)}"
    
    log_text = "\n".join(logs)
    
    if len(results) == 0:
        return None, None, log_text + "\n\nì²˜ë¦¬ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤"
    elif len(results) == 1:
        # 1ê°œ: ë°”ë¡œ ë¹„ë””ì˜¤ í‘œì‹œ
        return results[0][0], None, log_text
    else:
        # 2ê°œ ì´ìƒ: ZIP ìƒì„±
        temp_dir = tempfile.mkdtemp()
        zip_path = os.path.join(temp_dir, "photo_results.zip")
        with zipfile.ZipFile(zip_path, 'w') as zf:
            for video_path, name in results:
                zf.write(video_path, name)
        return None, zip_path, log_text + f"\n\nì´ {len(results)}ê°œ íŒŒì¼ì´ ZIPìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤"


# ============ ì˜ìƒ ë¹„êµ ============

def get_video_rotation(video_path):
    """ffprobeë¥¼ ì‚¬ìš©í•´ ì˜ìƒì˜ íšŒì „ ë©”íƒ€ë°ì´í„° í™•ì¸"""
    try:
        cmd = [
            'ffprobe', '-v', 'quiet', '-print_format', 'json',
            '-show_streams', '-select_streams', 'v:0', video_path
        ]
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0:
            data = json.loads(result.stdout)
            if 'streams' in data and len(data['streams']) > 0:
                stream = data['streams'][0]
                if 'side_data_list' in stream:
                    for side_data in stream['side_data_list']:
                        if 'rotation' in side_data:
                            return int(side_data['rotation'])
                if 'tags' in stream and 'rotate' in stream['tags']:
                    return int(stream['tags']['rotate'])
    except Exception as e:
        print(f"ffprobe error: {e}")
    return 0

def rotate_frame(frame, rotation):
    """íšŒì „ ê°ë„ì— ë”°ë¼ í”„ë ˆì„ íšŒì „"""
    if rotation == 90 or rotation == -270:
        return cv2.rotate(frame, cv2.ROTATE_90_COUNTERCLOCKWISE)
    elif rotation == -90 or rotation == 270:
        return cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)
    elif rotation == 180 or rotation == -180:
        return cv2.rotate(frame, cv2.ROTATE_180)
    return frame

def add_label(frame, text, position='top'):
    """í”„ë ˆì„ì— ë¼ë²¨ ì¶”ê°€"""
    h, w = frame.shape[:2]
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = max(0.8, min(w, h) / 500)
    thickness = max(2, int(font_scale * 2))
    
    (text_w, text_h), baseline = cv2.getTextSize(text, font, font_scale, thickness)
    
    padding = 10
    if position == 'top':
        x = (w - text_w) // 2
        y = text_h + padding + 20
    else:
        x = (w - text_w) // 2
        y = h - padding - 20
    
    cv2.rectangle(frame, (x - padding, y - text_h - padding), 
                  (x + text_w + padding, y + baseline + padding), (0, 0, 0), -1)
    cv2.putText(frame, text, (x, y), font, font_scale, (255, 255, 255), thickness)
    
    return frame

def get_middle_frame(cap, rotation):
    """ì¤‘ê°„ í”„ë ˆì„ ì¶”ì¶œ"""
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    middle_idx = total_frames // 2
    
    cap.set(cv2.CAP_PROP_POS_FRAMES, middle_idx)
    ret, frame = cap.read()
    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # ë¦¬ì…‹
    
    if ret:
        frame = rotate_frame(frame, rotation)
        return frame
    return None

def calculate_balanced_transforms(bi, ai):
    """ì–‘ìª½ ë°˜ë°˜ì”© ì¡°ì ˆí•˜ëŠ” ë³€í™˜ ê³„ì‚°"""
    is_half = ai['is_half_face']
    
    if is_half:
        # ë°˜ëª¨: ì •ì¤‘ì„  ê¸°ì¤€
        b_size = bi['midline_length']
        a_size = ai['midline_length']
        b_angle = bi['midline_angle']
        a_angle = ai['midline_angle']
        b_cx, b_cy = bi['midline_center_x'], bi['midline_center_y']
        a_cx, a_cy = ai['midline_center_x'], ai['midline_center_y']
    else:
        # ì•ˆëª¨: ëˆˆ ê¸°ì¤€
        b_size = bi['eye_dist']
        a_size = ai['eye_dist']
        b_angle = bi['eye_angle']
        a_angle = ai['eye_angle']
        b_cx, b_cy = bi['eyes_center_x'], bi['eyes_center_y']
        a_cx, a_cy = ai['eyes_center_x'], ai['eyes_center_y']
    
    # ì¤‘ê°„ê°’ ê³„ì‚°
    target_size = (b_size + a_size) / 2
    target_angle = (b_angle + a_angle) / 2
    
    # ê°ê°ì˜ ìŠ¤ì¼€ì¼ ê³„ì‚° (ì¤‘ê°„ í¬ê¸°ë¡œ ë§ì¶”ê¸°)
    scale_b = target_size / b_size if b_size > 0 else 1.0
    scale_a = target_size / a_size if a_size > 0 else 1.0
    
    # ìŠ¤ì¼€ì¼ ì œí•œ (0.5 ~ 2.0)
    scale_b = np.clip(scale_b, 0.5, 2.0)
    scale_a = np.clip(scale_a, 0.5, 2.0)
    
    # íšŒì „ ê°ë„ (ì¤‘ê°„ ê°ë„ë¡œ ë§ì¶”ê¸°)
    angle_b = -np.degrees(target_angle - b_angle)
    angle_a = -np.degrees(target_angle - a_angle)
    
    # íšŒì „ ì œí•œ
    angle_b = np.clip(angle_b, -15, 15)
    angle_a = np.clip(angle_a, -15, 15)
    
    return {
        'scale_b': scale_b,
        'scale_a': scale_a,
        'angle_b': angle_b,
        'angle_a': angle_a,
        'b_center': (b_cx, b_cy),
        'a_center': (a_cx, a_cy),
        'is_half': is_half
    }

def apply_transform(frame, scale, angle, center):
    """í”„ë ˆì„ì— ë³€í™˜ ì ìš©"""
    h, w = frame.shape[:2]
    cx, cy = center
    
    M = cv2.getRotationMatrix2D((cx, cy), angle, scale)
    
    # ë³€í™˜ í›„ ì¤‘ì‹¬ì´ í”„ë ˆì„ ì¤‘ì•™ì— ì˜¤ë„ë¡ ì¡°ì •
    new_cx = M[0,0]*cx + M[0,1]*cy + M[0,2]
    new_cy = M[1,0]*cx + M[1,1]*cy + M[1,2]
    
    M[0,2] += w/2 - new_cx
    M[1,2] += h/2 - new_cy
    
    return cv2.warpAffine(frame, M, (w, h), borderMode=cv2.BORDER_CONSTANT, borderValue=(255, 255, 255))

def process_single_video(before_path, after_path, add_labels_flag=True):
    """ë‹¨ì¼ ì˜ìƒ ìŒ ì²˜ë¦¬ (ì¤‘ê°„ í”„ë ˆì„ ë¶„ì„, ì–‘ìª½ ë°˜ë°˜ ìŠ¤ì¼€ì¼)"""
    
    rotation_b = get_video_rotation(before_path)
    rotation_a = get_video_rotation(after_path)
    
    cap_before = cv2.VideoCapture(before_path)
    cap_after = cv2.VideoCapture(after_path)
    
    if not cap_before.isOpened() or not cap_after.isOpened():
        return None, "ì˜ìƒì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
    
    fps_before = cap_before.get(cv2.CAP_PROP_FPS)
    fps_after = cap_after.get(cv2.CAP_PROP_FPS)
    fps = min(fps_before, fps_after, 30)
    
    frame_count_before = int(cap_before.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_count_after = int(cap_after.get(cv2.CAP_PROP_FRAME_COUNT))
    
    # ì¤‘ê°„ í”„ë ˆì„ì—ì„œ ì–¼êµ´ ì •ë³´ ì¶”ì¶œ
    mid_frame_b = get_middle_frame(cap_before, rotation_b)
    mid_frame_a = get_middle_frame(cap_after, rotation_a)
    
    if mid_frame_b is None or mid_frame_a is None:
        return None, "ì˜ìƒ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
    
    # ë¦¬ì‚¬ì´ì¦ˆ
    mid_frame_b = resize_if_needed(mid_frame_b, VIDEO_MAX_SIZE)
    mid_frame_a = resize_if_needed(mid_frame_a, VIDEO_MAX_SIZE)
    
    h_b, w_b = mid_frame_b.shape[:2]
    h_a, w_a = mid_frame_a.shape[:2]
    
    # ì–¼êµ´ ì •ë³´ ì¶”ì¶œ
    before_info = get_face_info(mid_frame_b)
    after_info = get_face_info(mid_frame_a)
    
    # ì–‘ìª½ ë°˜ë°˜ ë³€í™˜ ê³„ì‚°
    transforms = calculate_balanced_transforms(before_info, after_info)
    
    # ìµœì¢… ì¶œë ¥ í¬ê¸° (ë‘˜ ë‹¤ ë³€í™˜ í›„ ê°™ì€ í¬ê¸°ë¡œ)
    # ë³€í™˜ í›„ í¬ê¸°ë¥¼ ê³ ë ¤í•´ì„œ ê³µí†µ í¬ê¸° ê²°ì •
    target_h = max(h_b, h_a)
    target_w = max(w_b, w_a)
    
    # êµ¬ë¶„ì„  ë„ˆë¹„
    divider_width = 4
    
    # ìµœì¢… í¬ê¸° (16ì˜ ë°°ìˆ˜)
    panel_w = (target_w // 16) * 16
    panel_h = (target_h // 16) * 16
    final_w = panel_w * 2 + divider_width
    final_h = panel_h
    
    # ì¶œë ¥ ì„¤ì •
    temp_dir = tempfile.mkdtemp()
    output_path = os.path.join(temp_dir, "side_by_side.mp4")
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (final_w, final_h))
    
    max_frames = max(frame_count_before, frame_count_after)
    
    frame_idx = 0
    last_frame_b = None
    last_frame_a = None
    
    while frame_idx < max_frames:
        # Before í”„ë ˆì„
        if frame_idx < frame_count_before:
            ret_b, frame_b = cap_before.read()
            if ret_b:
                frame_b = rotate_frame(frame_b, rotation_b)
                last_frame_b = frame_b.copy()
        else:
            frame_b = last_frame_b
        
        # After í”„ë ˆì„
        if frame_idx < frame_count_after:
            ret_a, frame_a = cap_after.read()
            if ret_a:
                frame_a = rotate_frame(frame_a, rotation_a)
                last_frame_a = frame_a.copy()
        else:
            frame_a = last_frame_a
        
        if frame_b is None or frame_a is None:
            break
        
        # ë¦¬ì‚¬ì´ì¦ˆ
        frame_b = resize_if_needed(frame_b, VIDEO_MAX_SIZE)
        frame_a = resize_if_needed(frame_a, VIDEO_MAX_SIZE)
        
        # í˜„ì¬ í”„ë ˆì„ì˜ ì–¼êµ´ ì •ë³´ë¡œ ì¤‘ì‹¬ì  ì—…ë°ì´íŠ¸ (ì˜µì…˜ - ë” ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)
        # ì—¬ê¸°ì„œëŠ” ì²« í”„ë ˆì„ ê¸°ì¤€ ì‚¬ìš©
        
        # ë³€í™˜ ì ìš©
        frame_b_transformed = apply_transform(frame_b, transforms['scale_b'], 
                                               transforms['angle_b'], transforms['b_center'])
        frame_a_transformed = apply_transform(frame_a, transforms['scale_a'], 
                                               transforms['angle_a'], transforms['a_center'])
        
        # íŒ¨ë„ í¬ê¸°ë¡œ ì¡°ì •
        frame_b_panel = cv2.resize(frame_b_transformed, (panel_w, panel_h), interpolation=cv2.INTER_LANCZOS4)
        frame_a_panel = cv2.resize(frame_a_transformed, (panel_w, panel_h), interpolation=cv2.INTER_LANCZOS4)
        
        # ë¼ë²¨ ì¶”ê°€
        if add_labels_flag:
            frame_b_panel = add_label(frame_b_panel, "BEFORE", 'top')
            frame_a_panel = add_label(frame_a_panel, "AFTER", 'top')
        
        # í•©ì¹˜ê¸°
        combined = np.zeros((final_h, final_w, 3), dtype=np.uint8)
        combined[:, :panel_w] = frame_b_panel
        combined[:, panel_w:panel_w+divider_width] = [255, 255, 255]
        combined[:, panel_w+divider_width:] = frame_a_panel
        
        out.write(combined)
        frame_idx += 1
    
    cap_before.release()
    cap_after.release()
    out.release()
    
    duration = max_frames / fps
    detect_b = "âœ“" if before_info['detected'] else "âœ—"
    detect_a = "âœ“" if after_info['detected'] else "âœ—"
    align_type = "ë°˜ëª¨(ì •ì¤‘ì„ )" if transforms['is_half'] else "ì•ˆëª¨(ëˆˆ)"
    scale_info = f"ìŠ¤ì¼€ì¼: BÃ—{transforms['scale_b']:.2f}, AÃ—{transforms['scale_a']:.2f}"
    rot_info = f" | íšŒì „ë³´ì •: B({rotation_b}Â°) A({rotation_a}Â°)" if rotation_b != 0 or rotation_a != 0 else ""
    
    info = f"{final_w}Ã—{final_h} | {duration:.1f}ì´ˆ | {align_type}\n{scale_info}{rot_info}\nì–¼êµ´ê²€ì¶œ: ì „({detect_b}) í›„({detect_a})"
    
    return output_path, info

def create_video_comparison(before_files, after_files, add_labels_flag=True):
    """ì˜ìƒ ë°°ì¹˜ ì²˜ë¦¬"""
    if not before_files or not after_files:
        return None, None, "BEFOREì™€ AFTER ì˜ìƒì„ ëª¨ë‘ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”"
    
    # íŒŒì¼ ì •ë ¬
    before_list = sorted(before_files, key=lambda x: natural_sort_key(Path(x).name))
    after_list = sorted(after_files, key=lambda x: natural_sort_key(Path(x).name))
    
    if len(before_list) != len(after_list):
        return None, None, f"íŒŒì¼ ìˆ˜ê°€ ë‹¤ë¦…ë‹ˆë‹¤: BEFORE {len(before_list)}ê°œ, AFTER {len(after_list)}ê°œ"
    
    results = []
    logs = []
    
    for idx, (bf, af) in enumerate(zip(before_list, after_list)):
        bf_name = Path(bf).stem
        af_name = Path(af).stem
        logs.append(f"[{idx+1}/{len(before_list)}] {bf_name} â†” {af_name}")
        
        try:
            output_path, info = process_single_video(bf, af, add_labels_flag)
            if output_path:
                results.append((output_path, f"{bf_name}_{af_name}.mp4"))
                logs[-1] += f" âœ“"
                logs.append(f"    {info}")
            else:
                logs[-1] += f" âœ— {info}"
        except Exception as e:
            logs[-1] += f" âœ— ì˜¤ë¥˜: {str(e)}"
    
    log_text = "\n".join(logs)
    
    if len(results) == 0:
        return None, None, log_text + "\n\nì²˜ë¦¬ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤"
    elif len(results) == 1:
        return results[0][0], None, log_text
    else:
        temp_dir = tempfile.mkdtemp()
        zip_path = os.path.join(temp_dir, "video_results.zip")
        with zipfile.ZipFile(zip_path, 'w') as zf:
            for video_path, name in results:
                zf.write(video_path, name)
        return None, zip_path, log_text + f"\n\nì´ {len(results)}ê°œ íŒŒì¼ì´ ZIPìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤"


# ============ UI ============

custom_css = """
.gradio-container { max-width: 1000px !important; margin: auto !important; }
footer { display: none !important; }
"""

with gr.Blocks(title="Dental B&A", css=custom_css) as demo:
    gr.Markdown("<h1 style='text-align:center'>ğŸ¦· ì¹˜ê³¼ ì „í›„ ë¹„êµ</h1>")
    gr.Markdown("<p style='text-align:center;color:#666'>íŒŒì¼ëª… ìˆœì„œëŒ€ë¡œ ìë™ ë§¤ì¹­ (1â†”1, 2â†”2, ...)</p>")
    
    with gr.Tabs():
        # ===== íƒ­ 1: ì‚¬ì§„ ë¹„êµ =====
        with gr.TabItem("ğŸ“· ì‚¬ì§„ ë¹„êµ"):
            gr.Markdown("<p style='text-align:center;color:#888'>ì‚¬ì§„ â†’ ë””ì¡¸ë¸Œ MP4 ìƒì„±</p>")
            
            with gr.Row():
                photo_before = gr.File(label="BEFORE ì‚¬ì§„ë“¤", file_count="multiple", file_types=["image"])
                photo_after = gr.File(label="AFTER ì‚¬ì§„ë“¤", file_count="multiple", file_types=["image"])
            
            with gr.Accordion("ë¡œê³  ì¶”ê°€ (ì„ íƒ)", open=False):
                logo_input = gr.Image(label="PNG íˆ¬ëª… ë°°ê²½ ì§€ì›", type="numpy")
            
            photo_btn = gr.Button("ğŸ¬ ì˜ìƒ ìƒì„±", variant="primary", size="lg")
            
            with gr.Row():
                photo_video_out = gr.Video(label="ê²°ê³¼ (1ê°œì¼ ë•Œ)")
                photo_zip_out = gr.File(label="ZIP ë‹¤ìš´ë¡œë“œ (2ê°œ ì´ìƒ)")
            
            photo_log = gr.Textbox(label="ì²˜ë¦¬ ë¡œê·¸", lines=6)
            
            photo_btn.click(
                fn=create_photo_comparison,
                inputs=[photo_before, photo_after, logo_input],
                outputs=[photo_video_out, photo_zip_out, photo_log]
            )
        
        # ===== íƒ­ 2: ì˜ìƒ ë¹„êµ =====
        with gr.TabItem("ğŸ¬ ì˜ìƒ ë¹„êµ"):
            gr.Markdown("<p style='text-align:center;color:#888'>ì˜ìƒ â†’ ì¢Œìš° ë¹„êµ (ì¤‘ê°„ í”„ë ˆì„ ë¶„ì„, ì–‘ìª½ ë°˜ë°˜ ìŠ¤ì¼€ì¼)</p>")
            
            with gr.Row():
                video_before = gr.File(label="BEFORE ì˜ìƒë“¤", file_count="multiple", file_types=["video"])
                video_after = gr.File(label="AFTER ì˜ìƒë“¤", file_count="multiple", file_types=["video"])
            
            add_labels_check = gr.Checkbox(label="BEFORE/AFTER ë¼ë²¨ í‘œì‹œ", value=True)
            
            video_btn = gr.Button("ğŸ¬ ì˜ìƒ í•©ì¹˜ê¸°", variant="primary", size="lg")
            
            with gr.Row():
                video_video_out = gr.Video(label="ê²°ê³¼ (1ê°œì¼ ë•Œ)")
                video_zip_out = gr.File(label="ZIP ë‹¤ìš´ë¡œë“œ (2ê°œ ì´ìƒ)")
            
            video_log = gr.Textbox(label="ì²˜ë¦¬ ë¡œê·¸", lines=6)
            
            video_btn.click(
                fn=create_video_comparison,
                inputs=[video_before, video_after, add_labels_check],
                outputs=[video_video_out, video_zip_out, video_log]
            )

demo.launch()
