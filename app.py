import gradio as gr
import cv2
import numpy as np
import tempfile
import os
import zipfile

MAX_SIZE = 1920

# ===== Ïú†Ìã∏Î¶¨Ìã∞ =====
def resize_if_needed(img):
    h, w = img.shape[:2]
    if max(h, w) > MAX_SIZE:
        scale = MAX_SIZE / max(h, w)
        img = cv2.resize(img, None, fx=scale, fy=scale, interpolation=cv2.INTER_LANCZOS4)
    return img

def resize_and_crop_to_match(img1, img2):
    h1, w1 = img1.shape[:2]
    h2, w2 = img2.shape[:2]
    ratio1, ratio2 = w1/h1, w2/h2
    
    if ratio1 > ratio2:
        new_w = int(h2 * ratio1)
        new_h = h2
    else:
        new_w = w2
        new_h = int(w2 / ratio1)
    
    img1_resized = cv2.resize(img1, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)
    sx, sy = (new_w - w2) // 2, (new_h - h2) // 2
    return img1_resized[sy:sy+h2, sx:sx+w2]

def match_brightness(img1, img2):
    lab1 = cv2.cvtColor(img1, cv2.COLOR_BGR2LAB).astype(np.float32)
    lab2 = cv2.cvtColor(img2, cv2.COLOR_BGR2LAB).astype(np.float32)
    avg = (np.mean(lab1[:,:,0]) + np.mean(lab2[:,:,0])) / 2
    lab1[:,:,0] = np.clip(lab1[:,:,0] + (avg - np.mean(lab1[:,:,0])), 0, 255)
    lab2[:,:,0] = np.clip(lab2[:,:,0] + (avg - np.mean(lab2[:,:,0])), 0, 255)
    return cv2.cvtColor(lab1.astype(np.uint8), cv2.COLOR_LAB2BGR), cv2.cvtColor(lab2.astype(np.uint8), cv2.COLOR_LAB2BGR)

def add_logo(frame, logo, margin_ratio=0.03, width_ratio=0.27):
    if logo is None:
        return frame
    h, w = frame.shape[:2]
    logo_w = int(w * width_ratio)
    logo_h = int(logo.shape[0] * (logo_w / logo.shape[1]))
    logo_resized = cv2.resize(logo, (logo_w, logo_h), interpolation=cv2.INTER_LANCZOS4)
    margin = int(w * margin_ratio)
    
    if logo_resized.shape[2] == 4:
        alpha = logo_resized[:, :, 3:4] / 255.0
        roi = frame[margin:margin+logo_h, margin:margin+logo_w]
        frame[margin:margin+logo_h, margin:margin+logo_w] = (logo_resized[:,:,:3] * alpha + roi * (1-alpha)).astype(np.uint8)
    else:
        frame[margin:margin+logo_h, margin:margin+logo_w] = logo_resized
    return frame

def add_label(img, text, position='top'):
    h, w = img.shape[:2]
    font = cv2.FONT_HERSHEY_SIMPLEX
    scale = max(0.7, min(w, h) / 900)
    thickness = max(2, int(scale * 2))
    (tw, th), _ = cv2.getTextSize(text, font, scale, thickness)
    pad = 8
    bx = (w - tw - pad*2) // 2
    by = 12 if position == 'top' else h - th - pad*2 - 12
    cv2.rectangle(img, (bx, by), (bx + tw + pad*2, by + th + pad*2), (0,0,0), -1)
    cv2.putText(img, text, (bx + pad, by + th + pad), font, scale, (255,255,255), thickness)
    return img

# ===== Í∞êÏßÄ =====
def detect_face(img):
    """MediaPipe ÏñºÍµ¥ Í∞êÏßÄ"""
    try:
        import mediapipe as mp
        with mp.solutions.face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.3) as fm:
            results = fm.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
            if not results.multi_face_landmarks:
                return None
            
            lm = results.multi_face_landmarks[0].landmark
            h, w = img.shape[:2]
            
            left_eye = np.mean([[lm[i].x*w, lm[i].y*h] for i in [33,133,160,159,158,144,145,153]], axis=0)
            right_eye = np.mean([[lm[i].x*w, lm[i].y*h] for i in [362,263,387,386,385,373,374,380]], axis=0)
            
            eye_dist = np.linalg.norm(right_eye - left_eye)
            eye_angle = np.arctan2(right_eye[1]-left_eye[1], right_eye[0]-left_eye[0])
            eye_center = (left_eye + right_eye) / 2
            
            nose = np.array([lm[4].x*w, lm[4].y*h])
            chin = np.array([lm[152].x*w, lm[152].y*h])
            upper_lip = np.array([lm[0].x*w, lm[0].y*h])
            lower_lip = np.array([lm[17].x*w, lm[17].y*h])
            
            midline_len = np.linalg.norm(chin - nose)
            midline_angle = np.arctan2(chin[1]-nose[1], chin[0]-nose[0])
            midline_center = (upper_lip + lower_lip) / 2
            
            face_pts = [lm[i] for i in [10, 152, 234, 454]]
            xs = [p.x*w for p in face_pts]
            ys = [p.y*h for p in face_pts]
            aspect = (max(ys)-min(ys)) / (max(xs)-min(xs)) if max(xs)!=min(xs) else 1
            
            is_half = aspect < 0.9
            
            return {
                'type': 'half_face' if is_half else 'full_face',
                'eye_dist': eye_dist, 'eye_angle': eye_angle, 'eye_center': eye_center,
                'midline_len': midline_len, 'midline_angle': midline_angle, 'midline_center': midline_center
            }
    except Exception as e:
        print(f"Face detection error: {e}")
        return None

def detect_oral(img):
    """Íµ¨Í∞ï Í∞êÏßÄ - ÏπòÏùÄÏó∞ VÏûê Ìå®ÌÑ¥ÏúºÎ°ú Ï†ïÏ§ëÏÑ†(11-21 ÏÇ¨Ïù¥) Ï∞æÍ∏∞"""
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    h, w = img.shape[:2]
    
    # OtsuÎ°ú ÏπòÏïÑ ÎßàÏä§ÌÅ¨
    blurred = cv2.GaussianBlur(gray, (5,5), 0)
    _, teeth_mask = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    
    # Ï§ëÏïô ÏòÅÏó≠Îßå
    roi = np.zeros_like(teeth_mask)
    m = 0.12
    roi[int(h*m):int(h*(1-m)), int(w*m):int(w*(1-m))] = 255
    teeth_mask = cv2.bitwise_and(teeth_mask, roi)
    
    kernel = np.ones((5,5), np.uint8)
    teeth_mask = cv2.morphologyEx(teeth_mask, cv2.MORPH_CLOSE, kernel)
    
    # ÏÉÅÌïòÏïÖ Í≤ΩÍ≥Ñ
    row_sum = np.convolve(np.sum(teeth_mask>0, axis=1), np.ones(11)/11, mode='same')
    search = row_sum[int(h*0.35):int(h*0.65)]
    occlusal_y = int(h*0.35) + np.argmin(search) if len(search) > 0 else h//2
    
    # ÏÉÅÏïÖ ÏπòÏïÑÎßå
    upper_teeth = teeth_mask.copy()
    upper_teeth[occlusal_y:, :] = 0
    
    # Í∞Å Ïó¥ÏóêÏÑú ÏπòÏïÑ ÏÉÅÎã® (ÏπòÏùÄÏó∞) Ï∞æÍ∏∞
    gumline_y = []
    for x in range(w):
        col = upper_teeth[:, x]
        teeth_rows = np.where(col > 0)[0]
        if len(teeth_rows) > 20:
            gumline_y.append(teeth_rows[0])
        else:
            gumline_y.append(0)
    
    gumline_y = np.array(gumline_y)
    gumline_smooth = np.convolve(gumline_y, np.ones(9)/9, mode='same')
    
    # Ï§ëÏïô ÏòÅÏó≠ÏóêÏÑú VÏûê(Í∑πÎåÄÏ†ê) Ï∞æÍ∏∞
    center_start = int(w * 0.35)
    center_end = int(w * 0.65)
    center_gumline = gumline_smooth[center_start:center_end]
    
    peaks = []
    for i in range(5, len(center_gumline)-5):
        if center_gumline[i] > center_gumline[i-5] and center_gumline[i] > center_gumline[i+5]:
            if center_gumline[i] > 50:
                peaks.append((center_start + i, center_gumline[i]))
    
    # Í∞ÄÏû• Ï§ëÏïôÏóê Í∞ÄÍπåÏö¥ peak = 11-21 ÏÇ¨Ïù¥
    if peaks:
        mid_x = w // 2
        peaks.sort(key=lambda p: abs(p[0] - mid_x))
        midline_x = peaks[0][0]
    else:
        # Ìè¥Î∞±: ÏπòÏïÑ Î¨¥Í≤åÏ§ëÏã¨
        contours, _ = cv2.findContours(upper_teeth, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if contours:
            largest = max(contours, key=cv2.contourArea)
            M = cv2.moments(largest)
            midline_x = int(M['m10'] / M['m00']) if M['m00'] > 0 else w // 2
        else:
            midline_x = w // 2
    
    # ÏπòÏïÑ ÎÑàÎπÑ
    contours, _ = cv2.findContours(upper_teeth, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if contours:
        largest = max(contours, key=cv2.contourArea)
        x, y, bw, bh = cv2.boundingRect(largest)
        if cv2.contourArea(largest) < w*h*0.01:
            return None
        teeth_width = bw
    else:
        return None
    
    return {
        'type': 'oral',
        'cx': float(midline_x),
        'cy': float(occlusal_y),
        'width': float(teeth_width)
    }

def detect_features(img):
    """ÌÜµÌï© Í∞êÏßÄ"""
    face = detect_face(img)
    if face:
        return face
    
    oral = detect_oral(img)
    if oral:
        return oral
    
    h, w = img.shape[:2]
    return {'type': 'fallback', 'cx': w/2, 'cy': h/2, 'width': w*0.5}

# ===== Ï†ïÎ†¨ =====
def align_balanced(before, after, bi, ai):
    """Í∑†Ìòï Ïä§ÏºÄÏùºÎßÅ"""
    h, w = before.shape[:2]
    target = np.array([w/2.0, h/2.0])
    
    b_type, a_type = bi['type'], ai['type']
    
    # ÏïàÎ™®
    if b_type == 'full_face' and a_type == 'full_face':
        ratio = ai['eye_dist'] / bi['eye_dist']
        ratio = np.clip(ratio, 0.5, 2.0)
        mid_scale = np.sqrt(ratio)
        b_scale = np.clip(mid_scale, 0.8, 1.25)
        a_scale = np.clip(1/mid_scale, 0.8, 1.25)
        
        angle_diff = ai['eye_angle'] - bi['eye_angle']
        b_angle = np.degrees(angle_diff/2)
        a_angle = -np.degrees(angle_diff/2)
        
        b_anchor = np.array(bi['eye_center'], dtype=np.float64)
        a_anchor = np.array(ai['eye_center'], dtype=np.float64)
        align_type = "ÏïàÎ™®"
    
    # Î∞òÎ™®
    elif b_type == 'half_face' and a_type == 'half_face':
        ratio = ai['midline_len'] / bi['midline_len']
        ratio = np.clip(ratio, 0.5, 2.0)
        mid_scale = np.sqrt(ratio)
        b_scale = np.clip(mid_scale, 0.8, 1.25)
        a_scale = np.clip(1/mid_scale, 0.8, 1.25)
        
        angle_diff = ai['midline_angle'] - bi['midline_angle']
        b_angle = np.degrees(angle_diff/2)
        a_angle = -np.degrees(angle_diff/2)
        
        b_anchor = np.array(bi['midline_center'], dtype=np.float64)
        a_anchor = np.array(ai['midline_center'], dtype=np.float64)
        align_type = "Î∞òÎ™®"
    
    # Íµ¨Í∞ï
    elif b_type == 'oral' or a_type == 'oral':
        b_w = float(bi.get('width', w*0.5))
        a_w = float(ai.get('width', w*0.5))
        ratio = a_w / b_w
        ratio = np.clip(ratio, 0.5, 2.0)
        mid_scale = np.sqrt(ratio)
        b_scale = np.clip(mid_scale, 0.7, 1.4)
        a_scale = np.clip(1/mid_scale, 0.7, 1.4)
        
        b_anchor = np.array([float(bi.get('cx', w/2)), float(bi.get('cy', h/2))], dtype=np.float64)
        a_anchor = np.array([float(ai.get('cx', w/2)), float(ai.get('cy', h/2))], dtype=np.float64)
        b_angle, a_angle = 0.0, 0.0
        align_type = "Íµ¨Í∞ï"
    
    else:
        return before.copy(), after.copy(), "ÎØ∏Í∞êÏßÄ"
    
    # Î≥ÄÌôò
    def transform(img, anchor, angle, scale, target):
        M = cv2.getRotationMatrix2D((float(anchor[0]), float(anchor[1])), float(angle), float(scale))
        new_anchor = M @ np.array([anchor[0], anchor[1], 1.0])
        M[0,2] += target[0] - new_anchor[0]
        M[1,2] += target[1] - new_anchor[1]
        return cv2.warpAffine(img, M, (img.shape[1], img.shape[0]), borderMode=cv2.BORDER_REPLICATE)
    
    before_out = transform(before, b_anchor, b_angle, b_scale, target)
    after_out = transform(after, a_anchor, a_angle, a_scale, target)
    
    return before_out, after_out, f"{align_type}({b_scale:.2f}/{a_scale:.2f})"

# ===== Ï≤òÎ¶¨ =====
def process_images(before_img, after_img, logo_img=None):
    before = cv2.cvtColor(before_img, cv2.COLOR_RGB2BGR)
    after = cv2.cvtColor(after_img, cv2.COLOR_RGB2BGR)
    
    after = resize_if_needed(after)
    before = resize_if_needed(before)
    
    if before.shape[:2] != after.shape[:2]:
        before = resize_and_crop_to_match(before, after)
    
    logo = None
    if logo_img is not None:
        logo = cv2.cvtColor(logo_img, cv2.COLOR_RGBA2BGRA if logo_img.shape[2]==4 else cv2.COLOR_RGB2BGR)
    
    bi = detect_features(before)
    ai = detect_features(after)
    
    before_aligned, after_aligned, align_type = align_balanced(before, after, bi, ai)
    
    # ÌÅ¨Î°≠
    h, w = after_aligned.shape[:2]
    m = 0.05
    bf = before_aligned[int(h*m):int(h*(1-m)), int(w*m):int(w*(1-m))]
    af = after_aligned[int(h*m):int(h*(1-m)), int(w*m):int(w*(1-m))]
    
    bf, af = match_brightness(bf, af)
    
    th, tw = af.shape[:2]
    fw, fh = (tw//16)*16, (th//16)*16
    bf = cv2.resize(bf, (fw, fh), interpolation=cv2.INTER_LANCZOS4)
    af = cv2.resize(af, (fw, fh), interpolation=cv2.INTER_LANCZOS4)
    
    return bf, af, logo, align_type, fw, fh

# ===== Ï∂úÎ†• =====
def create_dissolve(before_img, after_img, logo_img=None):
    if before_img is None or after_img is None:
        return None, "Ï†Ñ/ÌõÑ ÏÇ¨ÏßÑ ÌïÑÏöî"
    
    try:
        bf, af, logo, align_type, fw, fh = process_images(before_img, after_img, logo_img)
        
        fps = 30
        frames = [39, 12, 39]
        
        temp = tempfile.mkdtemp()
        path = os.path.join(temp, "dissolve.mp4")
        out = cv2.VideoWriter(path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (fw, fh))
        
        for i in range(sum(frames)):
            if i < frames[0]:
                frame = bf.copy()
            elif i < frames[0] + frames[1]:
                alpha = (i - frames[0]) / frames[1]
                frame = cv2.addWeighted(bf, 1-alpha, af, alpha, 0)
            else:
                frame = af.copy()
            if logo is not None:
                frame = add_logo(frame, logo)
            out.write(frame)
        
        out.release()
        return path, f"{fw}√ó{fh} | 3.0Ï¥à | {align_type}"
    except Exception as e:
        import traceback
        return None, f"Ïò§Î•ò: {e}\n{traceback.format_exc()}"

def create_sidebyside(before_img, after_img, logo_img=None):
    if before_img is None or after_img is None:
        return None, "Ï†Ñ/ÌõÑ ÏÇ¨ÏßÑ ÌïÑÏöî"
    
    try:
        bf, af, logo, align_type, fw, fh = process_images(before_img, after_img, logo_img)
        
        bf_l = add_label(bf.copy(), "BEFORE")
        af_l = add_label(af.copy(), "AFTER")
        
        divider = np.ones((fh, 4, 3), dtype=np.uint8) * 255
        combined = np.hstack([bf_l, divider, af_l])
        
        if logo is not None:
            combined = add_logo(combined, logo, width_ratio=0.15)
        
        return cv2.cvtColor(combined, cv2.COLOR_BGR2RGB), f"{combined.shape[1]}√ó{combined.shape[0]} | {align_type}"
    except Exception as e:
        import traceback
        return None, f"Ïò§Î•ò: {e}\n{traceback.format_exc()}"

# ===== Î∞∞Ïπò =====
def process_batch(files, output_type, logo_img=None):
    if not files or len(files) < 2:
        return None, "ÏµúÏÜå 2Í∞ú ÌååÏùº ÌïÑÏöî"
    
    files = sorted(files, key=lambda x: x.name if hasattr(x, 'name') else x)
    
    if len(files) % 2 != 0:
        return None, "ÏßùÏàò Í∞úÏàò ÌïÑÏöî"
    
    temp = tempfile.mkdtemp()
    results = []
    
    for i in range(0, len(files), 2):
        before_path = files[i].name if hasattr(files[i], 'name') else files[i]
        after_path = files[i+1].name if hasattr(files[i+1], 'name') else files[i+1]
        
        before_img = cv2.cvtColor(cv2.imread(before_path), cv2.COLOR_BGR2RGB)
        after_img = cv2.cvtColor(cv2.imread(after_path), cv2.COLOR_BGR2RGB)
        
        idx = i // 2 + 1
        
        if output_type == "ÎîîÏ°∏Î∏å":
            result, _ = create_dissolve(before_img, after_img, logo_img)
            if result:
                new_path = os.path.join(temp, f"dissolve_{idx:03d}.mp4")
                os.rename(result, new_path)
                results.append(new_path)
        else:
            result, _ = create_sidebyside(before_img, after_img, logo_img)
            if result is not None:
                new_path = os.path.join(temp, f"sidebyside_{idx:03d}.png")
                cv2.imwrite(new_path, cv2.cvtColor(result, cv2.COLOR_RGB2BGR))
                results.append(new_path)
    
    if not results:
        return None, "Ï≤òÎ¶¨ Ïã§Ìå®"
    
    zip_path = os.path.join(temp, "batch_results.zip")
    with zipfile.ZipFile(zip_path, 'w') as zf:
        for r in results:
            zf.write(r, os.path.basename(r))
    
    return zip_path, f"{len(results)}Í∞ú ÏôÑÎ£å"

# ===== UI =====
with gr.Blocks(title="Dental B&A", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ü¶∑ ÏπòÍ≥º Ï†ÑÌõÑ ÎπÑÍµê")
    gr.Markdown("ÏïàÎ™®/Î∞òÎ™®/Íµ¨Í∞ï ÏûêÎèô Í∞êÏßÄ ‚Üí Ï†ïÏ§ëÏÑ† Ï†ïÎ†¨")
    
    with gr.Tabs():
        with gr.Tab("Îã®Ïùº Ï≤òÎ¶¨"):
            with gr.Row():
                before_input = gr.Image(label="BEFORE", type="numpy")
                after_input = gr.Image(label="AFTER", type="numpy")
            
            with gr.Accordion("Î°úÍ≥† (ÏÑ†ÌÉù)", open=False):
                logo_input = gr.Image(label="PNG", type="numpy")
            
            with gr.Row():
                dissolve_btn = gr.Button("üé¨ ÎîîÏ°∏Î∏å", variant="primary")
                sidebyside_btn = gr.Button("üñºÔ∏è Ï¢åÏö∞ÎπÑÍµê")
            
            with gr.Row():
                video_out = gr.Video(label="ÏòÅÏÉÅ")
                image_out = gr.Image(label="Ïù¥ÎØ∏ÏßÄ")
            
            status_out = gr.Textbox(label="Ï†ïÎ≥¥")
            
            dissolve_btn.click(create_dissolve, [before_input, after_input, logo_input], [video_out, status_out])
            sidebyside_btn.click(create_sidebyside, [before_input, after_input, logo_input], [image_out, status_out])
        
        with gr.Tab("Î∞∞Ïπò Ï≤òÎ¶¨"):
            gr.Markdown("ÌååÏùºÎ™ÖÏàú before/after ÏåçÏúºÎ°ú Ï≤òÎ¶¨")
            
            batch_files = gr.File(label="Ïù¥ÎØ∏ÏßÄÎì§", file_count="multiple", file_types=["image"])
            batch_type = gr.Radio(["ÎîîÏ°∏Î∏å", "Ï¢åÏö∞ÎπÑÍµê"], value="ÎîîÏ°∏Î∏å", label="Ï∂úÎ†•")
            batch_logo = gr.Image(label="Î°úÍ≥†", type="numpy")
            
            batch_btn = gr.Button("Î∞∞Ïπò ÏãúÏûë", variant="primary")
            batch_out = gr.File(label="Í≤∞Í≥º ZIP")
            batch_status = gr.Textbox(label="ÏÉÅÌÉú")
            
            batch_btn.click(process_batch, [batch_files, batch_type, batch_logo], [batch_out, batch_status])

demo.launch()
